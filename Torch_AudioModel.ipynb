{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"Torch_AudioModel.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"1VGJYtJ5ktnm"},"source":["!pip install torchaudio\n","!pip install pydub"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VUfJhCSNlKz1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607451610073,"user_tz":-180,"elapsed":22179,"user":{"displayName":"Mert Yazan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9Fx1HeSPjkufTps2J0icn5Vi6YdnbSZ3_Y_kWdA=s64","userId":"09195293638216274039"}},"outputId":"1a81d6c7-5ac3-4fa9-e8da-e9e3091bcd58"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PHMncMVok0Lv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607447994972,"user_tz":-180,"elapsed":13108,"user":{"displayName":"Mert Yazan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9Fx1HeSPjkufTps2J0icn5Vi6YdnbSZ3_Y_kWdA=s64","userId":"09195293638216274039"}},"outputId":"e4601d57-2b2d-4c5c-d5eb-1b071c2f0a27"},"source":["from torch.utils import data\n","import torchaudio\n","import os\n","from pydub import AudioSegment\n","import numpy as np\n","import pydub\n","import torch\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn import *\n","from torch.optim import *\n","from tqdm import tqdm\n","import copy\n","from sklearn.metrics import classification_report\n","from torch import nn\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n","  '\"sox\" backend is being deprecated. '\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"BYf1IIXilpa5"},"source":["os.chdir(\"/content/drive/My Drive\")\n","\n","!cp Audio_Pad.zip /content\n","!cp Eklenti.zip /content\n","!cp AudioSet.zip /content\n","\n","os.chdir(\"/content\")\n","\n","!unzip Audio_Pad.zip\n","!unzip Eklenti.zip\n","!unzip AudioSet.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o8NAuKk6B6n1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607448379828,"user_tz":-180,"elapsed":1664,"user":{"displayName":"Mert Yazan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9Fx1HeSPjkufTps2J0icn5Vi6YdnbSZ3_Y_kWdA=s64","userId":"09195293638216274039"}},"outputId":"4add74c0-3e7a-40db-a41b-2fae50bd307b"},"source":["!mv -v Eklenti/Train/Speech/* Audio_Pad/Train/Speech\n","!mv -v Eklenti/Train/Non_Speech/* Audio_Pad/Train/Non_Speech\n","!mv -v Eklenti/Val/Speech/* Audio_Pad/Val/Speech\n","!mv -v Eklenti/Val/Non_Speech/* Audio_Pad/Val/Non_Speech"],"execution_count":null,"outputs":[{"output_type":"stream","text":["mv: cannot stat 'Eklenti/Train/Speech/*': No such file or directory\n","mv: cannot stat 'Eklenti/Train/Non_Speech/*': No such file or directory\n","mv: cannot stat 'Eklenti/Val/Speech/*': No such file or directory\n","mv: cannot stat 'Eklenti/Val/Non_Speech/*': No such file or directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"oBBT1hWGkW2W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607448682912,"user_tz":-180,"elapsed":95535,"user":{"displayName":"Mert Yazan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9Fx1HeSPjkufTps2J0icn5Vi6YdnbSZ3_Y_kWdA=s64","userId":"09195293638216274039"}},"outputId":"7f385b99-53d8-4a14-bd28-090220395bdb"},"source":["sp_path = \"Audio_Pad/Train/Speech\"\n","\n","nonsp_path = \"Audio_Pad/Train/Non_Speech\"\n","\n","sp_tensor = torch.empty((len(os.listdir(sp_path)),128, 157))\n","nonsp_tensor = torch.empty((len(os.listdir(nonsp_path)),128, 157))\n","\n","i = 0\n","\n","for song in os.listdir(sp_path):\n","    sound, _ = torchaudio.load(sp_path + \"/\" + song, channels_first = False)\n","    sound = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128, n_fft = 2048)(sound.flatten())\n","    sound = torchaudio.transforms.FrequencyMasking(freq_mask_param = 15)(sound)\n","    sound = torchaudio.transforms.TimeMasking(time_mask_param = 35)(sound)\n","    \n","    sp_tensor[i] = sound\n","    i += 1\n","\n","i = 0\n","for song in os.listdir(nonsp_path):\n","    sound, _ = torchaudio.load(nonsp_path + \"/\" + song, channels_first = False)\n","    sound = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128, n_fft = 2048)(sound.flatten())\n","    sound = torchaudio.transforms.FrequencyMasking(freq_mask_param = 15)(sound)\n","    sound = torchaudio.transforms.TimeMasking(time_mask_param = 35)(sound)\n","    \n","    nonsp_tensor[i] = sound\n","    i += 1\n","    \n","train_data = torch.cat((sp_tensor, nonsp_tensor))\n","\n","train_labels = torch.cat((torch.ones(len(os.listdir(sp_path))), torch.zeros(len(os.listdir(nonsp_path)))))\n","\n","sp_tensor = []\n","nonsp_tensor = []\n","\n","del sp_tensor\n","del nonsp_tensor\n","\n","aug_train_data = torch.zeros((50,128,157))\n","aug_train_labels = torch.zeros((50))\n","\n","train_data = torch.cat((train_data, aug_train_data))\n","train_labels = torch.cat((train_labels,aug_train_labels))\n","\n","train_data = train_data[:,None, ...]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/functional.py:516: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n","  normalized, onesided, return_complex)\n","/usr/local/lib/python3.6/dist-packages/torch/functional.py:516: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n","  normalized, onesided, return_complex)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"C1lfNy0KkW2Z"},"source":["sp_path = \"Audio_Pad/Val/Speech\"\n","\n","nonsp_path = \"Audio_Pad/Val/Non_Speech\"\n","\n","sp_tensor = torch.empty((len(os.listdir(sp_path)),128, 157))\n","nonsp_tensor = torch.empty((len(os.listdir(nonsp_path)),128, 157))\n","\n","i = 0\n","\n","for song in os.listdir(sp_path):\n","    sound, _ = torchaudio.load(sp_path + \"/\" + song, channels_first = False)\n","    sound = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128, n_fft = 2048)(sound.flatten())\n","    \n","    sp_tensor[i] = sound\n","    i += 1\n","\n","i = 0\n","for song in os.listdir(nonsp_path):\n","    sound, _ = torchaudio.load(nonsp_path + \"/\" + song, channels_first = False)\n","    sound = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128, n_fft = 2048)(sound.flatten())\n","    \n","    nonsp_tensor[i] = sound\n","    i += 1\n","    \n","val_data = torch.cat((sp_tensor, nonsp_tensor))\n","\n","val_labels = torch.cat((torch.ones(len(os.listdir(sp_path))), torch.zeros(len(os.listdir(nonsp_path)))))\n","\n","sp_tensor = []\n","nonsp_tensor = []\n","\n","del sp_tensor\n","del nonsp_tensor\n","\n","val_data = val_data[:,None, ...]\n","                                                                                              \n","                                                                                              \n","sp_path = \"Audio_Pad/Test/Speech\"\n","\n","nonsp_path = \"Audio_Pad/Test/Non_Speech\"\n","\n","sp_tensor = torch.empty((len(os.listdir(sp_path)),128, 157))\n","nonsp_tensor = torch.empty((len(os.listdir(nonsp_path)),128, 157))\n","\n","i = 0\n","\n","for song in os.listdir(sp_path):\n","    sound, _ = torchaudio.load(sp_path + \"/\" + song, channels_first = False)\n","    sound = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128, n_fft = 2048)(sound.flatten())\n","    sound = torchaudio.transforms.FrequencyMasking(freq_mask_param = 15)(sound)\n","    sound = torchaudio.transforms.TimeMasking(time_mask_param=35)(sound)\n","    \n","    sp_tensor[i] = sound\n","    i += 1\n","\n","i = 0\n","for song in os.listdir(nonsp_path):\n","    sound, _ = torchaudio.load(nonsp_path + \"/\" + song, channels_first = False)\n","    sound = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128, n_fft = 2048)(sound.flatten())\n","    sound = torchaudio.transforms.FrequencyMasking(freq_mask_param = 15)(sound)\n","    sound = torchaudio.transforms.TimeMasking(time_mask_param=35)(sound)\n","    \n","    nonsp_tensor[i] = sound\n","    i += 1\n","    \n","test_data = torch.cat((sp_tensor, nonsp_tensor))\n","\n","test_labels = torch.cat((torch.ones(len(os.listdir(sp_path))), torch.zeros(len(os.listdir(nonsp_path)))))\n","\n","sp_tensor = []\n","nonsp_tensor = []\n","\n","del sp_tensor\n","del nonsp_tensor\n","\n","test_data = test_data[:,None, ...]\n","\n","train_data = torch.cat((train_data, test_data))\n","\n","train_labels = torch.cat((train_labels, test_labels))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q7WpA1tiZ3A2","executionInfo":{"status":"ok","timestamp":1607448808167,"user_tz":-180,"elapsed":1302,"user":{"displayName":"Mert Yazan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9Fx1HeSPjkufTps2J0icn5Vi6YdnbSZ3_Y_kWdA=s64","userId":"09195293638216274039"}},"outputId":"28b619a3-5d95-47d6-90d6-dac4a6fb3927"},"source":["train_data.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10866, 1, 128, 157])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rnlS9i6EanJk","executionInfo":{"status":"ok","timestamp":1607448825676,"user_tz":-180,"elapsed":1407,"user":{"displayName":"Mert Yazan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9Fx1HeSPjkufTps2J0icn5Vi6YdnbSZ3_Y_kWdA=s64","userId":"09195293638216274039"}},"outputId":"25b1382d-0e6a-4069-a2cd-efafc26294f7"},"source":["val_data.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4641, 1, 128, 157])"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C33ikDcKaunq","executionInfo":{"status":"ok","timestamp":1607448873757,"user_tz":-180,"elapsed":1388,"user":{"displayName":"Mert Yazan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9Fx1HeSPjkufTps2J0icn5Vi6YdnbSZ3_Y_kWdA=s64","userId":"09195293638216274039"}},"outputId":"9c3a1a38-6b65-4561-bba5-10654f947773"},"source":["train_labels.sum()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(5192.)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"xtwV4Jh2dW0L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607275397611,"user_tz":-180,"elapsed":1213,"user":{"displayName":"Mert Yazan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9Fx1HeSPjkufTps2J0icn5Vi6YdnbSZ3_Y_kWdA=s64","userId":"09195293638216274039"}},"outputId":"72bdf351-80f2-467e-e149-8fc8143b35ef"},"source":["class CustomDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, data, label):\n","        self.data = torch.tensor(data, dtype=torch.float)\n","        self.label = torch.tensor(label, dtype=torch.long)\n","        \n","\n","    def __len__(self):\n","        return len(self.label)\n","\n","    def __getitem__(self, index):\n","\n","        item_data = self.data[index]\n","        item_label = self.label[index]\n","\n","        return item_data, item_label\n","\n","training_set = CustomDataset(train_data, train_labels)\n","training_generator = DataLoader(training_set, batch_size = 128, shuffle = True)\n","\n","val_set = CustomDataset(val_data, val_labels)\n","val_generator = DataLoader(training_set, batch_size = 128, shuffle = True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  after removing the cwd from sys.path.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \"\"\"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"8eCs_3KAkW2P"},"source":["class CNNLayerNorm(Module):\n","    \"\"\"Layer normalization built for cnns input\"\"\"\n","    def __init__(self, n_feats):\n","        super(CNNLayerNorm, self).__init__()\n","        self.layer_norm = LayerNorm(n_feats)\n","\n","    def forward(self, x):\n","        # x (batch, channel, feature, time)\n","        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n","        x = self.layer_norm(x)\n","        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n","\n","class BidirectionalGRU(nn.Module):\n","\n","    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n","        super(BidirectionalGRU, self).__init__()\n","\n","        self.BiGRU = nn.GRU(\n","            input_size=rnn_dim, hidden_size=hidden_size,\n","            num_layers=1, batch_first=batch_first, bidirectional=True)\n","        self.layer_norm = nn.LayerNorm(rnn_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x = self.layer_norm(x)\n","        x = F.gelu(x)\n","        x, _ = self.BiGRU(x)\n","        x = self.dropout(x)\n","        return x\n","\n","class SoundDetectorModel(Module):   \n","    def __init__(self):\n","        super(SoundDetectorModel, self).__init__()\n","          \n","\n","        self.sound_detector_model =  Sequential(\n","            Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n","            ReLU(inplace=True),\n","            CNNLayerNorm(128),\n","            Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n","            ReLU(inplace=True),\n","            CNNLayerNorm(128),\n","            MaxPool2d(kernel_size=2, stride=2),\n","            Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n","            ReLU(inplace=True),\n","            CNNLayerNorm(64),\n","            Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n","            ReLU(inplace=True),\n","            CNNLayerNorm(64),\n","            MaxPool2d(kernel_size=2, stride=2),\n","            Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            ReLU(inplace=True),\n","            MaxPool2d(kernel_size=2, stride=2),\n","            Dropout(0.5),\n","            Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n","            ReLU(inplace=True),\n","            Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n","            ReLU(inplace=True),\n","            MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n","            Dropout(0.5),\n","            Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n","            ReLU(inplace=True),\n","            Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n","            ReLU(inplace=True),\n","            MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n","            Dropout(0.5)\n","            )\n","        \n","        self.classifier = nn.Sequential(\n","            ##BidirectionalGRU(1024, 512, 0.1, True),\n","            ##BidirectionalGRU(1024, 512, 0.1, True),\n","            Flatten(),\n","            Linear(16384, 2048), \n","            ReLU(inplace=True),\n","            LayerNorm(2048),\n","            Linear(2048, 2)\n","        )\n","            \n","\n","    def forward(self, x):\n","\n","        x = self.sound_detector_model(x)\n","        \n","        sizes = x.size()\n","        x = x.view(sizes[0], sizes[2] * sizes[3], sizes[1])\n","        ##x = x.transpose(1, 2)\n","        x = self.classifier(x)\n","\n","        \n","        return x\n","\n","\n","model = SoundDetectorModel()\n","\n","if torch.cuda.is_available():\n","    model = model.cuda()\n","\n","criterion = CrossEntropyLoss()\n","\n","optimizer = AdamW(model.parameters(), lr=0.0001)\n","\n","scheduler = lr_scheduler.OneCycleLR(optimizer,\n","\tmax_lr=0.0004,\n","\tsteps_per_epoch=int(len(training_generator)),\n","\tepochs=50,\n","\tanneal_strategy='linear')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nNAqeezxsXFd","executionInfo":{"status":"ok","timestamp":1607279214829,"user_tz":-180,"elapsed":816,"user":{"displayName":"Mert Yazan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9Fx1HeSPjkufTps2J0icn5Vi6YdnbSZ3_Y_kWdA=s64","userId":"09195293638216274039"}},"outputId":"c3791fdf-befc-4cde-af51-af6613247841"},"source":["model(val_data[0][None,...].cuda()).shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 2])"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"id":"ukNlufxtkW2t","colab":{"base_uri":"https://localhost:8080/","height":934},"executionInfo":{"status":"error","timestamp":1607283078753,"user_tz":-180,"elapsed":674194,"user":{"displayName":"Mert Yazan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9Fx1HeSPjkufTps2J0icn5Vi6YdnbSZ3_Y_kWdA=s64","userId":"09195293638216274039"}},"outputId":"796953f1-6d65-4e52-b4d4-38aaca2a24a3"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","# number of epochs to train the model\n","n_epochs = 50\n","\n","best_acc = 0.0\n","best_model_wts = copy.deepcopy(model.state_dict())\n","val_acc_history = []\n","\n","\n","for epoch in tqdm(range(1, n_epochs+1)):\n","    \n","    for phase in ['train', 'val']:\n","        \n","        if phase == 'train':\n","            model.train()  # Set model to training mode\n","        else:\n","            model.eval()   # Set model to evaluate mode\n","\n","        # keep track of training and validation loss\n","        running_loss = 0.0\n","        running_corrects = 0\n"," \n","\n","        for inputs, labels in training_generator:\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            optimizer.zero_grad()\n","            with torch.set_grad_enabled(phase == 'train'):\n","\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","                \n","                _, preds = torch.max(outputs, 1)\n","\n","                if phase == 'train':\n","                    loss.backward()\n","                    optimizer.step()           \n","\n","            running_loss += loss.item() * inputs.size(0)\n","            running_corrects += torch.sum(preds == labels.data)\n","\n","        epoch_loss = running_loss / len(training_generator.dataset)\n","        epoch_acc = running_corrects.double() / len(training_generator.dataset)\n","\n","        print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","    \n","        if phase == 'val' and epoch_acc > best_acc:\n","            best_acc = epoch_acc\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","        if phase == 'val':\n","            val_acc_history.append(epoch_acc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","\n","  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["train Loss: 0.0344 Acc: 0.9894\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","  2%|▏         | 1/50 [01:02<50:45, 62.15s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["val Loss: 0.0408 Acc: 0.9835\n","train Loss: 0.0323 Acc: 0.9893\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","  4%|▍         | 2/50 [02:05<49:53, 62.37s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["val Loss: 0.0343 Acc: 0.9901\n","train Loss: 0.0339 Acc: 0.9890\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","  6%|▌         | 3/50 [03:08<49:02, 62.60s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["val Loss: 0.0420 Acc: 0.9826\n","train Loss: 0.0268 Acc: 0.9925\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","  8%|▊         | 4/50 [04:11<48:06, 62.75s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["val Loss: 0.0478 Acc: 0.9808\n","train Loss: 0.0306 Acc: 0.9906\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n"," 10%|█         | 5/50 [05:14<47:08, 62.86s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["val Loss: 0.0433 Acc: 0.9863\n","train Loss: 0.0375 Acc: 0.9869\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n"," 12%|█▏        | 6/50 [06:17<46:09, 62.94s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["val Loss: 0.0281 Acc: 0.9915\n","train Loss: 0.0254 Acc: 0.9920\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n"," 14%|█▍        | 7/50 [07:20<45:08, 62.99s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["val Loss: 0.0488 Acc: 0.9798\n","train Loss: 0.0303 Acc: 0.9905\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n"," 16%|█▌        | 8/50 [08:23<44:07, 63.03s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["val Loss: 0.0429 Acc: 0.9817\n","train Loss: 0.0281 Acc: 0.9901\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n"," 18%|█▊        | 9/50 [09:26<43:05, 63.06s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["val Loss: 0.0532 Acc: 0.9827\n","train Loss: 0.0243 Acc: 0.9924\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n"," 20%|██        | 10/50 [10:29<42:02, 63.06s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["val Loss: 0.0233 Acc: 0.9934\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-59-7939195e0e87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"PBWbmHQ11wiG"},"source":["def preprocess_audio(data):\n","    \n","    if isinstance(data, str):\n","    \n","        audio = AudioSegment.from_file(data)\n","        audio = audio.set_frame_rate(16000)\n","        audio = audio.set_channels(1)\n","        audio = audio.get_array_of_samples()\n","        \n","        \n","    elif isinstance(data, np.ndarray) or isinstance(data, torch.Tensor):\n","        \n","        audio = np.array(data)\n","        \n","        if len(audio.shape) > 2:\n","            raise ValueError(\"Ses dizisinin boyutu en fazla 2 olabilir!\")\n","        elif len(audio.shape) == 2:\n","            if audio.shape[0] == 2:\n","                torch.reshape((audio.shape[1], audio.shape[0]))\n","            if isinstance(data, torch.Tensor):\n","                audio = np.mean(audio.numpy(), axis = 1)\n","            else:\n","                audio = np.mean(audio, axis = 1)\n","            \n","    aud_tensor = torch.tensor(np.array(audio))\n","    \n","    if aud_tensor.shape[0] < 160000:\n","        pad_shape = 160000 - aud_tensor.shape[0]\n","        pad_tensor = torch.zeros(pad_shape)\n","        aud_tensor = torch.cat((aud_tensor, pad_tensor))\n","        \n","    elif aud_tensor.shape[0] > 160000:\n","        splitted_tensor = list(torch.split(aud_tensor, 160000))\n","        if splitted_tensor[-1].shape[0] < 160000:\n","            pad_shape = 160000 - splitted_tensor[-1].shape[0]\n","            if aud_tensor.dtype == torch.int16:\n","                pad_tensor = torch.zeros(pad_shape, dtype = torch.int16)\n","            else:\n","                pad_tensor = torch.zeros(pad_shape)\n","            splitted_tensor[-1] = torch.cat((splitted_tensor[-1], pad_tensor)) \n","        aud_tensor = torch.stack(splitted_tensor)\n","     \n","    \n","    if aud_tensor.dtype == torch.int16:\n","        aud_tensor = torch.tensor(aud_tensor, dtype = torch.float)\n","        \n","    torchaudio.save(\"deneme.wav\", aud_tensor, sample_rate = 16000)\n","    aud_tensor = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128, n_fft = 2048)(aud_tensor)\n","    \n","    if len(aud_tensor.shape) == 2:\n","        aud_tensor = aud_tensor[None, None, ...]\n","    elif len(aud_tensor.shape) == 3:\n","        aud_tensor = aud_tensor[:,None,...]\n","    \n","    return aud_tensor\n","\n","def predict(path):\n","\n","    song = preprocess_audio(path)\n","    out = model(song.cuda())\n","\n","    _, preds = torch.max(out, 1)\n","\n","    \n","    if torch.sum(preds) > 0:\n","        print(\"Konuşma var!\")\n","    else:\n","        print(\"Konuşma yok!\")\n","    \n","    return torch.sum(preds)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tr6W_4ji6gat","executionInfo":{"status":"ok","timestamp":1607282381915,"user_tz":-180,"elapsed":1580,"user":{"displayName":"Mert Yazan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9Fx1HeSPjkufTps2J0icn5Vi6YdnbSZ3_Y_kWdA=s64","userId":"09195293638216274039"}},"outputId":"240ae188-1d40-4e67-8019-705b84017ad8"},"source":["model.load_state_dict(best_model_wts)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZR1-RcJS1Sdd","executionInfo":{"status":"ok","timestamp":1607282392369,"user_tz":-180,"elapsed":10266,"user":{"displayName":"Mert Yazan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9Fx1HeSPjkufTps2J0icn5Vi6YdnbSZ3_Y_kWdA=s64","userId":"09195293638216274039"}},"outputId":"0f017df3-a99c-4d0c-eca5-4cf3a381afe6"},"source":["model.eval()\n","\n","path = \"AudioSet/Non_Speech\"\n","len_dir = len(os.listdir(path))\n","true = 0\n","results = {}\n","\n","for song in os.listdir(path):\n","  with torch.no_grad():\n","    res = predict(path+ \"/\" + song)    \n","    if res == 1:\n","        print(song)\n","    else:\n","        true += 1\n","        \n","nonsp_acc = true/len_dir\n","\n","path = \"AudioSet/Speech\"\n","len_dir = len(os.listdir(path))\n","true = 0\n","results = {}\n","\n","for song in os.listdir(path):\n","  with torch.no_grad():\n","    res = predict(path+ \"/\" + song)    \n","    if res == 0:\n","        print(song)\n","    else:\n","        true += 1\n","        \n","sp_acc = true/len_dir\n","print(\"Speech Accuracy:\",sp_acc,\"Non_Speech Accuracy:\",nonsp_acc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"],"name":"stderr"},{"output_type":"stream","text":["Konuşma var!\n","8021.wav\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma var!\n","8284.wav\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma var!\n","8500.wav\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma var!\n","8522.wav\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma var!\n","8289.wav\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma var!\n","7123.wav\n","Konuşma yok!\n","Konuşma var!\n","8506.wav\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma var!\n","7030.wav\n","Konuşma yok!\n","Konuşma var!\n","7034.wav\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma var!\n","7015.wav\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma var!\n","7075.wav\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma var!\n","8022.wav\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma var!\n","8513.wav\n","Konuşma yok!\n","Konuşma var!\n","8544.wav\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma var!\n","7011.wav\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma var!\n","8297.wav\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma var!\n","8042.wav\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma var!\n","8204.wav\n","Konuşma var!\n","8283.wav\n","Konuşma yok!\n","Konuşma var!\n","7021.wav\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma var!\n","8530.wav\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma var!\n","8508.wav\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma var!\n","7010.wav\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma yok!\n","Konuşma var!\n","7087.wav\n","Konuşma yok!\n","8049.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","8269.wav\n","Konuşma yok!\n","8072.wav\n","Konuşma yok!\n","7000.wav\n","Konuşma var!\n","Konuşma yok!\n","8014.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","7062.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","8295.wav\n","Konuşma yok!\n","7093.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","8217.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","7107.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","7005.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","8240.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","8516.wav\n","Konuşma yok!\n","8529.wav\n","Konuşma yok!\n","8003.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","8066.wav\n","Konuşma var!\n","Konuşma yok!\n","8076.wav\n","Konuşma var!\n","Konuşma yok!\n","8075.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","8245.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","7091.wav\n","Konuşma yok!\n","8234.wav\n","Konuşma var!\n","Konuşma yok!\n","8019.wav\n","Konuşma yok!\n","8045.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","7103.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","8213.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","8090.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","8238.wav\n","Konuşma yok!\n","7097.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","8062.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","8210.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","8510.wav\n","Konuşma yok!\n","8229.wav\n","Konuşma var!\n","Konuşma yok!\n","8071.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","7076.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","8519.wav\n","Konuşma yok!\n","7104.wav\n","Konuşma yok!\n","8237.wav\n","Konuşma yok!\n","8053.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","7008.wav\n","Konuşma var!\n","Konuşma yok!\n","8504.wav\n","Konuşma var!\n","Konuşma yok!\n","7048.wav\n","Konuşma var!\n","Konuşma yok!\n","8069.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","8228.wav\n","Konuşma yok!\n","7121.wav\n","Konuşma yok!\n","8052.wav\n","Konuşma var!\n","Konuşma yok!\n","7082.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","8214.wav\n","Konuşma var!\n","Konuşma yok!\n","7137.wav\n","Konuşma yok!\n","7211.wav\n","Konuşma yok!\n","8220.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","7207.wav\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma var!\n","Konuşma yok!\n","8063.wav\n","Konuşma yok!\n","8036.wav\n","Konuşma var!\n","Konuşma var!\n","Speech Accuracy: 0.6580645161290323 Non_Speech Accuracy: 0.852760736196319\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uth7TZRvkW24"},"source":["torch.save(best_model_wts, \"SoundDetector.pth\")\n","\n","torch.save(model,'SoundDetector.pt')"],"execution_count":null,"outputs":[]}]}